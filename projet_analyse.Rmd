---
title: "projet"
output:
  word_document: default
  html_document: default
  pdf_document: default
date: "2023-04-11"
---


## Importation des données
```{r}
data <- read.csv("data/Train_data.csv")
head(data)
summary(data)


```

## Preparation du dataset

```{r}

#transform categorical variable into factor
data$class<-as.factor(data$class)
data$protocol_type<-as.factor(data$protocol_type)
data$flag<-as.factor(data$flag)
#data$service<-as.factor(data$service)

#faire doublons et outliers


```




```{r}
table(data$land) #refere to if the connection comes from the intranet
table(data$class[data$land == 1])


#The hypotheses of the Fisher's exact test are the same than for the Chi-square test, that is: H0 : the variables are independent, there is no relationship between the two categorical variables.
tab.cont.land <- xtabs(~class+land,data=data)
resultat.fisher.land <- fisher.test(tab.cont.land)
resultat.fisher.land


table(data$num_shells)
table(data$class[data$num_shells == 1])

tab.cont.num_shells <- xtabs(~class+num_shells,data=data)
resultat.fisher.num_shells <- fisher.test(tab.cont.num_shells)
resultat.fisher.num_shells



table(data$num_outbound_cmds)
table(data$is_host_login)

library(dplyr)
```

La taille des échantillons sont petites nous allons donc utiliser le test exact de fisher.

Pour land la p-value est supérieur à 0.05 donc nous conservons H0.On considère que land ne dépend pas de class. Nous allons donc retier la variable du dataset.

Pour num_shells la p-value est inférieur au seil de 5% nous pouvons donc rejeter H0 et considérons que num_shells depend de class.


Les variables num_outbound_cmds et is_host_login sont constantes nous pouvons donc les retier.


```{r}
data <- subset(data, select = -land)
data <- subset(data, select = -num_outbound_cmds)
data <- subset(data, select = -is_host_login)
data <- subset(data, select = -num_shells)
```

```{r}
length(unique(data$service))
#because service has 62 categories and random forest supports up to 53 we tranform the variable into numerical 
#we will do label encoding
data$service <- as.numeric(factor(data$service))


#essayer de mettre 66 categories

```

# Analyse descriptive

```{r}

mytable <- table(data[,"class"])
prop_table <- prop.table(mytable)
label_percentages <- paste(names(prop_table), round(prop_table * 100, 1), "%", sep = " ")
pie(mytable, labels = label_percentages, col = c("red","green"))

```

g

```{r}
barplot(table(data[,"flag"]),main="flag")
pie(table(data[,"protocol_type"]))

```



Nous séparons le dataset en 2 pour avoir une partie pour entrainer les modèles et une autre pour les tester
Normal est égal à 1

```{r}
set.seed(5678)
perm <- sample(4601,3000)
app <- data[perm,]
valid <- data[-perm,]

app2<-app
valid2 <- valid
app2$service<-as.factor(app2$service)
valid2$service<-as.factor(valid2$service)

app2$class <- ifelse(app2$class == "normal", 1, 0)
valid2$class <- ifelse(valid2$class == "normal", 1, 0)


app
app2
```

#Classification non supervisé
##kmean

Les variables du dataset on des unités différentes, nous devons donc réaliser une réduction sur les données. Pour effectuer la méthode des k-mean, nous précisons que nous voulons 2 classes car nous cherchons à déterminer si la connection est malveillante ou non.
```{r}
library(FactoMineR)
set.seed(1)
app2.cr <- scale(app[,c(1,3,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37)],center = TRUE, scale = TRUE)
kmean.result <- kmeans(app2.cr, centers = 2, nstart = 1000)
app.kmean <- cbind.data.frame(app2.cr,classe=factor(kmean.result$cluster))

catdes(app.kmean, num.var = 36)
kmean.table <- table(app2$class,kmean.result$cluster)
kmean.table
kmean.accuracy <- (kmean.table[1,2]+kmean.table[2,1])/3000
kmean.accuracy
```
Ce qui caractérise le plus les connections normales sont des same_srv_rate et dst_host_same_srv_rate élevés alors que ce qui caractérise le plus les connections malveillantes sont serror_rate, dst_host_srv_serror_rate et srv_serror_rate élevé ce qui paraît logique car un niveau d'erreur élevé du serveur client et host élevé est souvent associé à un problème.

##CAH

```{r}
d.app2.cr <- dist(app2.cr)
cah.ward <- hclust(d.app2.cr, method = "ward.D2")
plot(cah.ward )
groups.cah <- cutree(cah.ward, 2)
plot
```

```{r}
cah.table <- table(app2$class,groups.cah)
cah.accuracy <- 1 - (cah.table[1,1]+cah.table[2,2])/3000
cah.accuracy
```

# Classification supervisé

# LDA

```{r}
library(MASS)
model.lda <- lda(class~.,data=app ) 
model.lda
plot(model.lda)

```


## Prédiction

```{r}
pred_lda <- predict(model.lda,newdata=valid)$class

```

## Accuracy

```{r}

accuracy.lda = mean(pred_lda==valid$class)
accuracy.lda

```
## aire sous courbe ROC
```{r}
library("pROC")
pred_numeric_lda <- predict(model.lda,newdata=valid)$posterior[,2]
auc.lda <- roc(valid$class, pred_numeric_lda)$auc
auc.lda
```

# Arbre Optimal

```{r}
library(rpart)
library(rpart.plot)
set.seed(1)
arbre <- rpart(class~. ,app,control=rpart.control(minsplit=5,cp=0))
printcp(arbre)

```




```{r}
plotcp(arbre)
```
```{r}
cp.opt <- arbre$cptable[which.min(arbre$cptable[, "xerror"]), "CP"] 
arbre.opt <- prune(arbre,cp.opt) 
rpart.plot(arbre.opt, type=4)
```
## Prediction
```{r}
pred_arbre <- predict(arbre.opt,newdata=valid, type="class")

```
## Accuracy
```{r}
accuracy.arbre = mean(pred_arbre==valid$class)
accuracy.arbre
```

## aire sous courbe ROC
```{r}
library("pROC")
pred_numeric <- predict(arbre.opt,newdata=valid)[,2]
auc.rf <- roc(valid$class, pred_numeric)$auc
auc.rf
```



# random forest

```{r}
library("randomForest")
set.seed(1234)
model.rf <- randomForest(class~.,data=app)
model.rf
plot(model.rf)
tail(model.rf$err.rate)
```
## Prediction
```{r}
pred_forest <- predict(model.rf,newdata=valid, type="class")
table(pred_forest,valid$class)
```
## Accuracy
```{r}
accuracy.arbre = mean(pred_forest==valid$class)
accuracy.arbre
```

## validation
```{r}
library("pROC")
pred_numeric <- predict(model.rf, valid, type="prob")[,2]
auc.rf <- roc(valid$class, pred_numeric)$auc
auc.rf
```
```{r}
var.imp<-model.rf$importance
ord <- order(var.imp,decreasing = TRUE)
barplot(sort(var.imp,decreasing = TRUE)[1:12], names.arg=rownames(var.imp)[ord][1:12],cex.names=0.6, las=2)
```



```{r}
var.imp <- as.data.frame(var.imp)
var.imp.df <- cbind(variables = rownames(var.imp),var.imp)
var.imp.df <- var.imp[order(var.imp$MeanDecreaseGini, decreasing = TRUE),]
rownames(var.imp.df) <- NULL
```

### src_bytes 	number of data bytes from source to destination 

### "dst_bytes" typically refers to the number of bytes in the payload of a network packet received by the destination (receiver). It is a field or attribute commonly used in network traffic analysis and monitoring.







# gradient boosting



```{r}
mod.ada<-gbm(class~., data=app2, distribution="adaboost", shrinkage=0.01,n.trees=3000,cv.folds = 5)


```

```{r}
print(mod.ada)
```

```{r}
library("pROC")
prev.ada <- predict(mod.ada,newdata=valid,type = "response")
prev.ada <- round(prev.ada)
auc.ada <- roc(valid2$class, prev.ada)$auc
auc.ada

```

```{r}
summary(mod.ada)[1:10,]

```
# regression Logistique


```{r}
library(glmnet)
res_Lasso <- glmnet(as.matrix(app2[,-38]),app2$class,family = 'binomial')
plot(res_Lasso , label = TRUE)
plot(res_Lasso, xvar = "lambda", label = TRUE)
```
```{r}
cvLasso <- cv.glmnet(as.matrix(app2[,-38]),app2$class,family="binomial",
type.measure = "class")
cvLasso$lambda.min
```

```{r}
class_logit_lasso=predict(cvLasso, newx = as.matrix(data.test2[,-12]), s = 'lambda.min', type = "class")

```
```{r}
table(class_logit_lasso,data.test2$Heart_Healthy)
```

```{r}
accuracy_logit_lasso = mean(class_logit_lasso == data.test2$Heart_Healthy)
accuracy_logit_lasso
```

```{r}
mod.reg <- glm(class ~ ., family = binomial , data=app2)
```



```{r}
library(glmnet)

lasso <- glmnet(as.matrix(app[,1:37]),app[,38],family="binomial",alpha=0)
```



```{r}
#model.log.asso <- glmnet(as.matrix(app[,-1]),app$class,family='binomial')
#cvLasso <- cv.glmnet(as.matrix(app[,-1]),app$class,family="binomial", type.measure = "class")
#plot(cvLasso)
```


```{r}
plot(auc.ada,xlim=c(1,0))

```







